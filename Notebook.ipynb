{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My dataset loader and all my tested models including my final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_file, label_file=None, transform=None, train=True):\n",
    "        self.images = pd.read_csv(image_file)\n",
    "        self.labels = pd.read_csv(label_file) if label_file else None\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images.iloc[idx, :].values.astype(np.float32).reshape(28, 28)\n",
    "        image = image / 255.0\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.train and self.labels is not None:\n",
    "            label = self.labels.iloc[idx].values[0]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "\n",
    "\n",
    "# class FullyConnectedNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(FullyConnectedNN, self).__init__()\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(784, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 10),\n",
    "#             nn.LogSoftmax(dim=1)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 28*28)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "    \n",
    "\n",
    "\n",
    "# class ConvolutionalNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvolutionalNN, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(128 * 7 * 7, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 10),\n",
    "#             nn.LogSoftmax(dim=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "    \n",
    "\n",
    "\n",
    "# class ComplexCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ComplexCNN, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.conv4 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(256 * 7 * 7, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 10),\n",
    "#             nn.LogSoftmax(dim=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.conv4(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "# class LargeFeatureCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LargeFeatureCNN, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, padding=2),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(128 * 7 * 7, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 10),\n",
    "#             nn.LogSoftmax(dim=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "# class ResNetCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ResNetCNN, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.res_block = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256)\n",
    "#         )\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(256 * 7 * 7, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, 10),\n",
    "#             nn.LogSoftmax(dim=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "\n",
    "#         res = x\n",
    "#         x = self.res_block(x)\n",
    "#         x += res\n",
    "\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "    \n",
    "\n",
    "\n",
    "class FinalCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FinalCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting up the training data into train, test, and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "X = train_data.drop('label', axis=1)\n",
    "y = train_data['label']\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, \n",
    "                                                            test_size=0.1, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size=0.1111, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting and loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_data = CustomDataset('X_train.csv', 'y_train.csv', transform=transform)\n",
    "test_data = CustomDataset('X_test.csv', 'y_test.csv', transform=transform)\n",
    "val_data = CustomDataset('X_val.csv', 'y_val.csv', transform=transform)\n",
    "sub_test_data = CustomDataset('test.csv', transform=transform, train=False)\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "sub_test_loader = DataLoader(sub_test_data, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = FinalCNN().to(device)\n",
    "\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs =  []\n",
    "best_val_acc = 0\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    tot = 0\n",
    "\n",
    "    # Training Step\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Set parameter gradients to zero\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # Tracking the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Tracking the accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        tot += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = correct / tot\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation Step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_features, val_labels in val_loader:\n",
    "            val_features, val_labels = val_features.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_features)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "            \n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.3f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.3f}%')\n",
    "    \n",
    "    if epoch == 0 or val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'finalcnn_model.pth')\n",
    "        print('new best!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model on the final test set and prepare for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model = FinalCNN().to(device)\n",
    "model.load_state_dict(torch.load('finalcnn_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for images in sub_test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Prepare the results in the correct format\n",
    "submission_df = pd.DataFrame({\n",
    "    'ImageID': range(1, len(preds) + 1),\n",
    "    'label': preds\n",
    "})\n",
    "\n",
    "# submission_df.to_csv('final_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some code I made to visualise the kernels of my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_kernels(conv_layer, num_input_channels=32, num_output_channels=64):\n",
    "    # Copy over the weights of the convolution layer\n",
    "    kernels = conv_layer.weight.data.cpu()  \n",
    "\n",
    "    # Normalize kernels for visualization\n",
    "    min_wt = kernels.min()\n",
    "    max_wt = kernels.max()\n",
    "    kernels = (kernels - min_wt) / (max_wt - min_wt)\n",
    "\n",
    "    # Creating figure for the feature plot \n",
    "    fig, axs = plt.subplots(num_output_channels, num_input_channels, figsize=(num_input_channels, num_output_channels))\n",
    "\n",
    "    # Loop for each output channel\n",
    "    for i in range(num_output_channels):\n",
    "        # Loop for each input channel\n",
    "        for j in range(num_input_channels): \n",
    "            # Gets the kernel for each input-output pair\n",
    "            kernel = kernels[i, j].squeeze() \n",
    "            axs[i, j].imshow(kernel, cmap='gray')\n",
    "            axs[i, j].axis('off') \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "model = FinalCNN().to('cpu')\n",
    "model.load_state_dict(torch.load('finalcnn_model.pth'))\n",
    "visualise_kernels(model.conv2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
